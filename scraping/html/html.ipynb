{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import pkg_resources\n",
    "from packaging import version\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def get_latest_version(package):\n",
    "    try:\n",
    "        result = subprocess.check_output([sys.executable, '-m', 'pip', 'index', 'versions', package.split('[')[0]]).decode('utf-8')\n",
    "        versions = [line.split(' ')[-1] for line in result.split('\\n') if 'Available versions:' in line]\n",
    "        if versions:\n",
    "            return versions[0]\n",
    "    except subprocess.CalledProcessError:\n",
    "        return None\n",
    "\n",
    "def check_and_install_package(package):\n",
    "    package_name = package.split('[')[0]  # Ignorar extras para verificar a versão\n",
    "    try:\n",
    "        # Verificar se o pacote já está instalado e obter sua versão\n",
    "        installed_version = pkg_resources.get_distribution(package_name).version\n",
    "        print(f\"{package} já está instalado na versão {installed_version}. Verificando atualizações...\")\n",
    "        \n",
    "        # Verificar a versão mais recente disponível\n",
    "        latest_version = get_latest_version(package_name)\n",
    "        \n",
    "        if latest_version and version.parse(installed_version) < version.parse(latest_version):\n",
    "            print(f\"Atualizando {package} para a versão {latest_version}...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', package])\n",
    "        else:\n",
    "            print(f\"{package} já está atualizado.\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        # Se o pacote não está instalado, instalar\n",
    "        print(f\"Instalando {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "def install_and_update_packages(packages):\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(check_and_install_package, package) for package in packages]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "# Lista de pacotes necessários\n",
    "required_packages = [\n",
    "    'beautifulsoup4',\n",
    "    'requests',\n",
    "    'httpx[http2]',\n",
    "    'selenium',\n",
    "    'webdriver-manager',\n",
    "    'undetected-chromedriver',\n",
    "    'setuptools',\n",
    "    'rich',\n",
    "    'IPython',\n",
    "    'ipywidgets',\n",
    "    'selenium-wire',\n",
    "    'blinker'\n",
    "]\n",
    "\n",
    "install_and_update_packages(required_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "def get_html_with_requests(url):\n",
    " # Extrair domínio e caminho da URL\n",
    "  \n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "\n",
    "    headers = {\n",
    "        ':authority': domain,\n",
    "        ':method': 'GET',\n",
    "        ':path': path,\n",
    "        ':scheme': 'https',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br, zstd',\n",
    "        'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7,ru;q=0.6,es;q=0.5',\n",
    "        'Cookie': 'ajs_anonymous_id=a03ba2e0-6658-4732-bca0-5c82a729610d; blueID=158f8c36-f884-4eeb-9dd2-41ddb36954f2; _fbp=fb.2.1719565497536.945089496421286543; _tt_enable_cookie=1; _ttp=yDIqAjbcrTkStMgfYKWUtQqV7qI; _scid=fe8f87fb-9164-4128-931f-6db8349a5902; _pin_unauth=dWlkPU16QmxZV015TVRndE16QmhOUzAwWmpRekxUZzROakF0Wm1VeFlUSTROREE1WXpJeg; _hjSessionUser_257734=eyJpZCI6IjZlYmVkNTczLWRlNDItNTljNy1iYjBkLWRjMzQxYzg1ZThkZiIsImNyZWF0ZWQiOjE3MTk1NjU0OTc1ODksImV4aXN0aW5nIjp0cnVlfQ==; __utmz=other; blueULC=blue; OptanonAlertBoxClosed=2024-06-28T10:07:39.896Z; _gac_UA-58792402-1=1.1719578340.CjwKCAjwvvmzBhA2EiwAtHVrbx5PuZ-Y6ZmDRgxSQAE3bhfdMkfQ6qO_h_InopnIh9t314DXR2UOdhoCJPMQAvD_BwE; origem=adwords; _rtbhouse_source_=AdWords; anonymousClient=true; _gcl_aw=GCL.1719608537.CjwKCAjwvvmzBhA2EiwAtHVrb7a4Gf1uUaMKzAcQ623DOKsXA0ynRueURMbfhKPxXrTvmq46s3W5ZBoCXV8QAvD_BwE; _gcl_gs=2.1.k1$i1719608535; _gid=GA1.3.1821486782.1720470235; _ScCbts=%5B%5D; petzCarrinho=true; JSESSIONID=9E56A4F6251336BC2EEC9640328DF1CE; _hjSession_257734=eyJpZCI6ImRmYjFhNzkxLTcwMDAtNDc5ZC1hZDA2LTE4NGYwZGMwOTQ3MSIsImMiOjE3MjA2MzEyODk1MzEsInMiOjAsInIiOjAsInNiIjowLCJzciI6MCwic2UiOjAsImZzIjowLCJzcCI6MH0=; OptanonConsent=isGpcEnabled=0&datestamp=Wed+Jul+10+2024+14%3A33%3A28+GMT-0300+(Hor%C3%A1rio+Padr%C3%A3o+de+Bras%C3%ADlia)&version=202404.1.0&browserGpcFlag=0&isIABGlobal=false&hosts=&consentId=5d44d058-e0ca-4634-af6a-6ae970855f35&interactionCount=2&isAnonUser=1&landingPath=NotLandingPage&groups=C0001%3A1%2CC0003%3A1%2CC0002%3A1%2CC0004%3A1&AwaitingReconsent=false&intType=1&geolocation=BR%3BRJ; AWSALB=hZab3iIPYsD0MIUknEJjbESldllV2XbXHeW6ABubXDqQC1w+fB9ShoCYGJIR2z0ndrcA5n/bdu5qP2SU+sm1uOdvT/LotUC6UCm2meSI5Mm4DIVUR9N7D5oqkEID; AWSALBCORS=hZab3iIPYsD0MIUknEJjbESldllV2XbXHeW6ABubXDqQC1w+fB9ShoCYGJIR2z0ndrcA5n/bdu5qP2SU+sm1uOdvT/LotUC6UCm2meSI5Mm4DIVUR9N7D5oqkEID; _ga_JB2JHD7FCJ=GS1.1.1720631306.23.1.1720632809.57.0.0; _ga=GA1.1.604824160.1719565115; _uetsid=006ad4a03d6811ef82e9db6f4cebf44a; _uetvid=7e097870352d11ef8996917c57f63d87; _scid_r=fe8f87fb-9164-4128-931f-6db8349a5902; _derived_epik=dj0yJnU9OVpXTXl0T0xlTmRISjQ3aGwtVFQ1SWZjVmJuT0JiaFkmbj1zc3Z6NkNFQXNmdnZHWmFDdEpIX093Jm09MSZ0PUFBQUFBR2FPeGVzJnJtPTEmcnQ9QUFBQUFHYU94ZXMmc3A9Mg; cto_bundle=Eb4Xml9POEdtRUtaTE12dTVMYjVqT2xtMDIxaDE5blJXQWRHRkZXVEJlcUdNOTdFRGNwdGtMeWslMkZGVnBENzMwM0JYMDVmQkdIeVQ4NW1qMW9laEslMkZRQnl3dzElMkZPWFUlMkZmTFJxNGdLeXUlMkZTMTRqZyUyRndXSFZ4dEsxS1hyMk1ZeGpOQVU4bmU1bkFZY1lwcHp2M3NVZ1FWQWNmYmRYS0RIc1N1ZzFBcWRZcWNlOVcxRkVaeVZGd0YzQUxFMWglMkZheXhJa0cyJTJGdUg0NiUyQk92VCUyRnVZZmtxQWN6QVAlMkZzdyUzRCUzRA; ab.storage.sessionId.8160f757-f57f-4d17-9cfb-44c82d5e8f64=%7B%22g%22%3A%221c387921-d350-d759-9bb8-04a9e46b2359%22%2C%22e%22%3A1720632840493%2C%22c%22%3A1720632809379%2C%22l%22%3A1720632810493%7D; RT=\"z=1&dm=www.petz.com.br&si=92ca62e5-ee3d-405f-ab43-07f1cffe3709&ss=lyg3fxm8&sl=6&tt=cia&obo=2&rl=1&ld=zeiw&r=ke1m2ztx&ul=zeix\"',\n",
    "        'Priority': 'u=0, i',\n",
    "        'Referer': f\"{domain}/marcas\",\n",
    "        'Sec-Ch-Ua': '\"Not/A)Brand\";v=\"8\", \"Chromium\";v=\"126\", \"Google Chrome\";v=\"126\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Remover cabeçalhos pseudo para requests\n",
    "    headers = {k: v for k, v in headers.items() if not k.startswith(':')}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Erro ao obter HTML com requests: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_product_details(product_url, last_sku):\n",
    "    parsed_url = urlparse(product_url)\n",
    "    domain = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "    print(f\"Extraindo detalhes do produto da URL: {product_url}\")\n",
    "    html_content = get_html_with_requests(product_url)\n",
    "    if not html_content:\n",
    "        print(f\"Falha ao obter HTML para o produto: {product_url}\")\n",
    "        return None, last_sku\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Criar pasta para imagens se não existir\n",
    "    if not os.path.exists('imagens'):\n",
    "        os.makedirs('imagens')\n",
    "        print(\"Pasta 'imagens' criada.\")\n",
    "    \n",
    "    # Extrair categoria principal e subcategorias\n",
    "    breadcrumb = soup.find('nav', {'aria-label': 'breadcrumb'})\n",
    "    categories = []\n",
    "    if breadcrumb:\n",
    "        breadcrumb_items = breadcrumb.find_all('span', class_='breadcrumb-item')\n",
    "        if breadcrumb_items:\n",
    "            main_category = breadcrumb_items[0].find('meta', itemprop='name')['content']\n",
    "            subcategories = [item.find('meta', itemprop='name')['content'] for item in breadcrumb_items[1:]]\n",
    "            categories = {\n",
    "                \"name\": main_category,\n",
    "                \"link\": \"\",  # Link não disponível\n",
    "                \"subcategories\": [{\"name\": sub, \"link\": \"\"} for sub in subcategories]\n",
    "            }\n",
    "            print(f\"Categorias encontradas: {categories}\")\n",
    "        else:\n",
    "            print(\"Nenhuma categoria encontrada no breadcrumb.\")\n",
    "    else:\n",
    "        print(\"Breadcrumb não encontrado.\")\n",
    "    \n",
    "    # Extrair código de barras\n",
    "    barcode_element = soup.find('span', {'id': 'product-code'})\n",
    "    if barcode_element:\n",
    "        barcode = barcode_element.text.replace('Código: ', '').strip()\n",
    "        print(f\"Código de barras encontrado: {barcode}\")\n",
    "    else:\n",
    "        barcode = None\n",
    "        print(\"Código de barras não encontrado.\")\n",
    "    \n",
    "    # Extrair marca principal e submarca\n",
    "    brand_element = soup.find('a', {'itemprop': 'brand'})\n",
    "    if brand_element:\n",
    "        brand = brand_element.find('span', {'itemprop': 'name'}).text.strip()\n",
    "        print(f\"Marca encontrada: {brand}\")\n",
    "    else:\n",
    "        brand = None\n",
    "        print(\"Marca não encontrada.\")\n",
    "    \n",
    "    subbrand_element = soup.find('a', {'href': '/4-groomer'})\n",
    "    if subbrand_element:\n",
    "        subbrand = subbrand_element.find('span', class_='blue').text.strip()\n",
    "        print(f\"Submarca encontrada: {subbrand}\")\n",
    "    else:\n",
    "        subbrand = None\n",
    "        print(\"Submarca não encontrada.\")\n",
    "    \n",
    "    # Extrair descrição do produto\n",
    "    description_element = soup.find('section', {'class': 'description', 'id': 'description'})\n",
    "    if description_element:\n",
    "        description = description_element.find('div', class_='spec-content').text.strip()\n",
    "        print(f\"Descrição encontrada: {description}\")\n",
    "    else:\n",
    "        description = None\n",
    "        print(\"Descrição não encontrada.\")\n",
    "    \n",
    "    # Extrair especificações do produto\n",
    "    specifications = []\n",
    "    specifications_container = soup.find('div', class_='container')\n",
    "    if specifications_container:\n",
    "        spec_items = specifications_container.find_all('li', class_='specifications')\n",
    "        for item in spec_items:\n",
    "            spec_key = item.find('span', class_='spec-key').text.strip()\n",
    "            spec_value = item.find('span', class_='spec-value').text.strip()\n",
    "            specifications.append({\n",
    "                'key': spec_key,\n",
    "                'value': spec_value\n",
    "            })\n",
    "        print(f\"Especificações encontradas: {specifications}\")\n",
    "    else:\n",
    "        print(\"Especificações não encontradas.\")\n",
    "    \n",
    "    # Extrair variações do produto\n",
    "    variations = []\n",
    "    variations_container = soup.find('div', {'id': 'popupVariacoes'})\n",
    "    if variations_container:\n",
    "        print(\"Container de variações encontrado.\")\n",
    "        print(f\"HTML das variações:\\n{variations_container.prettify()}\\n\")\n",
    "        for variation in variations_container.find_all('div', class_='variacao-item'):\n",
    "            variation_name = variation.find('div', class_='item-name').text.strip()\n",
    "            price = variation.find('div', class_='modal-item-price').find('b').text.strip()\n",
    "            price = re.sub(r'[^\\d]', '', price)  # Remover caracteres não numéricos\n",
    "            barcode_variation = variation['data-code']\n",
    "            link_variation = domain + variation['data-urlvariacao']\n",
    "            last_sku += 1\n",
    "            print(f\"Extraindo detalhes da variação: {variation_name}, Preço: {price}, Código de barras: {barcode_variation}, Link: {link_variation}, SKU: {last_sku}\")\n",
    "            \n",
    "            variations.append({\n",
    "                'name': variation_name,\n",
    "                'price': price,\n",
    "                'barcode': barcode_variation,\n",
    "                'link': link_variation,\n",
    "                'sku': last_sku\n",
    "            })\n",
    "        print(f\"Variações encontradas: {variations}\")\n",
    "    else:\n",
    "        print(\"Nenhuma variação encontrada.\")\n",
    "        # Verificar se o preço está diretamente no HTML\n",
    "        price_element = soup.find('div', class_='current-price-left')\n",
    "        if price_element:\n",
    "            price = price_element.find('strong').text.strip()\n",
    "            price = re.sub(r'[^\\d]', '', price)  # Remover caracteres não numéricos\n",
    "            last_sku += 1\n",
    "            print(f\"Preço encontrado: {price}, SKU: {last_sku}\")\n",
    "            \n",
    "            variations.append({\n",
    "                'name': 'default',\n",
    "                'price': price,\n",
    "                'barcode': barcode,\n",
    "                'link': product_url,\n",
    "                'sku': last_sku\n",
    "            })\n",
    "        else:\n",
    "            print(\"Preço não encontrado.\")\n",
    "    \n",
    "    product_details = {\n",
    "        'categories': categories,\n",
    "        'barcode': barcode,\n",
    "        'brand': {\n",
    "            'name': brand,\n",
    "            'subbrand': {\n",
    "                'name': subbrand\n",
    "            } if subbrand else None\n",
    "        },\n",
    "        'description': description,\n",
    "        'specifications': specifications,\n",
    "        'variations': variations\n",
    "    }\n",
    "    \n",
    "    print(f\"Detalhes do produto extraídos: {product_details}\")\n",
    "    return product_details, last_sku\n",
    "\n",
    "def extract_products(html_content, marca_name, marca_link, last_sku):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    product_elements = soup.select('li.card-product.card-product-showcase[itemtype=\"http://schema.org/Product\"]')\n",
    "    \n",
    "    print(f\"Encontrados {len(product_elements)} elementos de produto para a marca '{marca_name}'\")\n",
    "    \n",
    "    products = []\n",
    "    for product in product_elements:\n",
    "        print(f\"HTML do produto:\\n{product.prettify()}\\n\")\n",
    "        \n",
    "        json_data = product.find('textarea', class_='jsonGa').text.strip()\n",
    "        product_info = json.loads(json_data)\n",
    "        \n",
    "        name = product_info['name']\n",
    "        link = product.find('meta', itemprop='url')['content']\n",
    "        \n",
    "        link = 'https://' + link\n",
    "        print(f\"Processando produto: {name}, URL: {link}\")\n",
    "        price = product_info['price']\n",
    "        product_id = product_info['id']\n",
    "        category = product_info['category']\n",
    "        brand = product_info['brand']\n",
    "        price_for_subs = product_info['priceForSubs']\n",
    "        hide_subscriber_discount_price = product_info['hideSubscriberDiscountPrice']\n",
    "        \n",
    "        product_data = {\n",
    "            \"name\": name,\n",
    "            \"link\": link,\n",
    "            \"price\": price,\n",
    "            \"id\": product_id,\n",
    "            \"category\": category,\n",
    "            \"brand\": brand,\n",
    "            \"priceForSubs\": price_for_subs,\n",
    "            \"hideSubscriberDiscountPrice\": hide_subscriber_discount_price\n",
    "        }\n",
    "        \n",
    "        # Obter detalhes adicionais do produto\n",
    "        product_details, last_sku = extract_product_details(link, last_sku)\n",
    "        if product_details:\n",
    "            product_data.update(product_details)\n",
    "        \n",
    "        products.append(product_data)\n",
    "        \n",
    "        # Inserir o produto no arquivo produtos.json\n",
    "        with open('produtos.json', 'r', encoding='utf-8') as f:\n",
    "            produtos_data = json.load(f)\n",
    "        \n",
    "        # Verifica se a marca já existe no produtos_data\n",
    "        existing_brand = next((m for m in produtos_data[\"marcas\"] if m[\"name\"] == marca_name), None)\n",
    "        \n",
    "        if existing_brand:\n",
    "            existing_brand[\"produtos\"].append(product_data)\n",
    "        else:\n",
    "            marca_data = {\n",
    "                \"name\": marca_name,\n",
    "                \"link\": marca_link,\n",
    "                \"produtos\": [product_data]\n",
    "            }\n",
    "            produtos_data[\"marcas\"].append(marca_data)\n",
    "        \n",
    "        # Salvar o arquivo produtos.json\n",
    "        with open('produtos.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(produtos_data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Produto '{name}' adicionado à marca '{marca_name}'\")\n",
    "    \n",
    "    return products, last_sku\n",
    "\n",
    "def process_brand(marca, last_sku):\n",
    "    url = marca['link']\n",
    "    print(f\"Processando marca: {marca['name']} - URL: {url}\")\n",
    "    try:\n",
    "        html_content = get_html_with_requests(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter HTML para a marca {marca['name']} - URL: {url} - Erro: {e}\")\n",
    "        return False, last_sku\n",
    "    \n",
    "    if html_content:\n",
    "        print(f\"HTML obtido para a marca {marca['name']}:\\n{html_content}\\n\")\n",
    "        products, last_sku = extract_products(html_content, marca['name'], url, last_sku)\n",
    "        print(f\"{len(products)} produtos processados para a marca '{marca['name']}'\")\n",
    "    else:\n",
    "        print(f\"Falha ao obter HTML para a marca {marca['name']} - URL: {url}\")\n",
    "        return False, last_sku\n",
    "    \n",
    "    return True, last_sku\n",
    "\n",
    "def main():\n",
    "    print(\"Iniciando o processo principal\")\n",
    "    \n",
    "    with open('marcas.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        print(\"Arquivo marcas.json carregado\")\n",
    "    \n",
    "    if 'subcategories' not in data:\n",
    "        print(\"Erro: A chave 'subcategories' não está presente no arquivo marcas.json\")\n",
    "        return\n",
    "    \n",
    "    # Criar o arquivo produtos.json se não existir\n",
    "    if not os.path.exists('produtos.json'):\n",
    "        with open('produtos.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump({\"marcas\": []}, f, ensure_ascii=False, indent=4)\n",
    "        print(\"Arquivo produtos.json criado\")\n",
    "    \n",
    "    # Adicionar as marcas ao arquivo produtos.json antes de processar os produtos\n",
    "    with open('produtos.json', 'r', encoding='utf-8') as f:\n",
    "        produtos_data = json.load(f)\n",
    "    \n",
    "    for marca in data['subcategories']:\n",
    "        if not any(m[\"name\"] == marca[\"name\"] for m in produtos_data[\"marcas\"]):\n",
    "            produtos_data[\"marcas\"].append({\n",
    "                \"name\": marca[\"name\"],\n",
    "                \"link\": marca[\"link\"],\n",
    "                \"produtos\": []\n",
    "            })\n",
    "            print(f\"Marca '{marca['name']}' adicionada ao arquivo produtos.json\")\n",
    "    \n",
    "    with open('produtos.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(produtos_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    # Obter o último SKU usado\n",
    "    last_sku = 0\n",
    "    for marca in produtos_data[\"marcas\"]:\n",
    "        for produto in marca[\"produtos\"]:\n",
    "            for variacao in produto.get(\"variations\", []):\n",
    "                if variacao[\"sku\"] > last_sku:\n",
    "                    last_sku = variacao[\"sku\"]\n",
    "    \n",
    "    # Processar todas as marcas\n",
    "    for marca in data['subcategories']:\n",
    "        _, last_sku = process_brand(marca, last_sku)\n",
    "    \n",
    "    print(\"Processo concluído\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_html_with_requests(url):\n",
    " # Extrair domínio e caminho da URL\n",
    "  \n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "\n",
    "    headers = {\n",
    "        ':authority': domain,\n",
    "        ':method': 'GET',\n",
    "        ':path': path,\n",
    "        ':scheme': 'https',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br, zstd',\n",
    "        'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7,ru;q=0.6,es;q=0.5',\n",
    "        'Cookie': 'ajs_anonymous_id=a03ba2e0-6658-4732-bca0-5c82a729610d; blueID=158f8c36-f884-4eeb-9dd2-41ddb36954f2; _fbp=fb.2.1719565497536.945089496421286543; _tt_enable_cookie=1; _ttp=yDIqAjbcrTkStMgfYKWUtQqV7qI; _scid=fe8f87fb-9164-4128-931f-6db8349a5902; _pin_unauth=dWlkPU16QmxZV015TVRndE16QmhOUzAwWmpRekxUZzROakF0Wm1VeFlUSTROREE1WXpJeg; _hjSessionUser_257734=eyJpZCI6IjZlYmVkNTczLWRlNDItNTljNy1iYjBkLWRjMzQxYzg1ZThkZiIsImNyZWF0ZWQiOjE3MTk1NjU0OTc1ODksImV4aXN0aW5nIjp0cnVlfQ==; __utmz=other; blueULC=blue; OptanonAlertBoxClosed=2024-06-28T10:07:39.896Z; _gac_UA-58792402-1=1.1719578340.CjwKCAjwvvmzBhA2EiwAtHVrbx5PuZ-Y6ZmDRgxSQAE3bhfdMkfQ6qO_h_InopnIh9t314DXR2UOdhoCJPMQAvD_BwE; origem=adwords; _rtbhouse_source_=AdWords; anonymousClient=true; _gcl_aw=GCL.1719608537.CjwKCAjwvvmzBhA2EiwAtHVrb7a4Gf1uUaMKzAcQ623DOKsXA0ynRueURMbfhKPxXrTvmq46s3W5ZBoCXV8QAvD_BwE; _gcl_gs=2.1.k1$i1719608535; _gid=GA1.3.1821486782.1720470235; _ScCbts=%5B%5D; petzCarrinho=true; JSESSIONID=9E56A4F6251336BC2EEC9640328DF1CE; _hjSession_257734=eyJpZCI6ImRmYjFhNzkxLTcwMDAtNDc5ZC1hZDA2LTE4NGYwZGMwOTQ3MSIsImMiOjE3MjA2MzEyODk1MzEsInMiOjAsInIiOjAsInNiIjowLCJzciI6MCwic2UiOjAsImZzIjowLCJzcCI6MH0=; OptanonConsent=isGpcEnabled=0&datestamp=Wed+Jul+10+2024+14%3A33%3A28+GMT-0300+(Hor%C3%A1rio+Padr%C3%A3o+de+Bras%C3%ADlia)&version=202404.1.0&browserGpcFlag=0&isIABGlobal=false&hosts=&consentId=5d44d058-e0ca-4634-af6a-6ae970855f35&interactionCount=2&isAnonUser=1&landingPath=NotLandingPage&groups=C0001%3A1%2CC0003%3A1%2CC0002%3A1%2CC0004%3A1&AwaitingReconsent=false&intType=1&geolocation=BR%3BRJ; AWSALB=hZab3iIPYsD0MIUknEJjbESldllV2XbXHeW6ABubXDqQC1w+fB9ShoCYGJIR2z0ndrcA5n/bdu5qP2SU+sm1uOdvT/LotUC6UCm2meSI5Mm4DIVUR9N7D5oqkEID; AWSALBCORS=hZab3iIPYsD0MIUknEJjbESldllV2XbXHeW6ABubXDqQC1w+fB9ShoCYGJIR2z0ndrcA5n/bdu5qP2SU+sm1uOdvT/LotUC6UCm2meSI5Mm4DIVUR9N7D5oqkEID; _ga_JB2JHD7FCJ=GS1.1.1720631306.23.1.1720632809.57.0.0; _ga=GA1.1.604824160.1719565115; _uetsid=006ad4a03d6811ef82e9db6f4cebf44a; _uetvid=7e097870352d11ef8996917c57f63d87; _scid_r=fe8f87fb-9164-4128-931f-6db8349a5902; _derived_epik=dj0yJnU9OVpXTXl0T0xlTmRISjQ3aGwtVFQ1SWZjVmJuT0JiaFkmbj1zc3Z6NkNFQXNmdnZHWmFDdEpIX093Jm09MSZ0PUFBQUFBR2FPeGVzJnJtPTEmcnQ9QUFBQUFHYU94ZXMmc3A9Mg; cto_bundle=Eb4Xml9POEdtRUtaTE12dTVMYjVqT2xtMDIxaDE5blJXQWRHRkZXVEJlcUdNOTdFRGNwdGtMeWslMkZGVnBENzMwM0JYMDVmQkdIeVQ4NW1qMW9laEslMkZRQnl3dzElMkZPWFUlMkZmTFJxNGdLeXUlMkZTMTRqZyUyRndXSFZ4dEsxS1hyMk1ZeGpOQVU4bmU1bkFZY1lwcHp2M3NVZ1FWQWNmYmRYS0RIc1N1ZzFBcWRZcWNlOVcxRkVaeVZGd0YzQUxFMWglMkZheXhJa0cyJTJGdUg0NiUyQk92VCUyRnVZZmtxQWN6QVAlMkZzdyUzRCUzRA; ab.storage.sessionId.8160f757-f57f-4d17-9cfb-44c82d5e8f64=%7B%22g%22%3A%221c387921-d350-d759-9bb8-04a9e46b2359%22%2C%22e%22%3A1720632840493%2C%22c%22%3A1720632809379%2C%22l%22%3A1720632810493%7D; RT=\"z=1&dm=www.petz.com.br&si=92ca62e5-ee3d-405f-ab43-07f1cffe3709&ss=lyg3fxm8&sl=6&tt=cia&obo=2&rl=1&ld=zeiw&r=ke1m2ztx&ul=zeix\"',\n",
    "        'Priority': 'u=0, i',\n",
    "        'Referer': f\"{domain}/marcas\",\n",
    "        'Sec-Ch-Ua': '\"Not/A)Brand\";v=\"8\", \"Chromium\";v=\"126\", \"Google Chrome\";v=\"126\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Remover cabeçalhos pseudo para requests\n",
    "    headers = {k: v for k, v in headers.items() if not k.startswith(':')}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Erro ao obter HTML com requests: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_images(html_content, sku, variations):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Extrair imagens padrão da div 'swiper-wrapper'\n",
    "        swiper_div = soup.find('div', {'class': 'swiper-wrapper'})\n",
    "        standard_images = set()\n",
    "        if swiper_div:\n",
    "            for img_tag in swiper_div.find_all('img', {'class': 'swiper-thumbnail-image'}):\n",
    "                img_url = img_tag.get('src')\n",
    "                if img_url:\n",
    "                    standard_images.add(img_url)\n",
    "        else:\n",
    "            print(\"Div 'swiper-wrapper' não encontrada.\")\n",
    "        \n",
    "        # Extrair imagens das variações\n",
    "        variation_images = {}\n",
    "        for variation in variations:\n",
    "            barcode = variation['barcode']\n",
    "            variation_images[barcode] = set()\n",
    "            for img_tag in soup.find_all('img', {'class': f'variation-{barcode}'}):  # Ajuste a classe conforme necessário\n",
    "                img_url = img_tag.get('src')\n",
    "                if img_url:\n",
    "                    variation_images[barcode].add(img_url)\n",
    "        \n",
    "        # Remover duplicatas\n",
    "        unique_standard_images = list(standard_images)\n",
    "        for images in variation_images.values():\n",
    "            unique_standard_images = list(set(unique_standard_images) - images)\n",
    "        \n",
    "        return unique_standard_images, variation_images\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair imagens: {e}\")\n",
    "        return [], {}\n",
    "\n",
    "def format_images(sku, standard_images, variation_images):\n",
    "    try:\n",
    "        formatted_images = {\n",
    "            \"imagesDefault\": [f\"{sku}_{idx}.jpg\" for idx in range(len(standard_images))],\n",
    "            \"variations\": []\n",
    "        }\n",
    "        \n",
    "        for barcode, images in variation_images.items():\n",
    "            formatted_images[\"variations\"].append({\n",
    "                \"name\": barcode,\n",
    "                \"images\": [f\"{barcode}_{idx}.jpg\" for idx in range(len(images))]\n",
    "            })\n",
    "        \n",
    "        return formatted_images\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao formatar imagens: {e}\")\n",
    "        return {}\n",
    "\n",
    "def download_images(image_urls, base_name, folder='imagens',referer=\"dd\"):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    for idx, image_url in enumerate(image_urls):\n",
    "        image_name = f\"{folder}/{base_name}_{idx}.jpg\" if len(image_urls) > 1 else f\"{folder}/{base_name}.jpg\"\n",
    "        print(f\"Baixando imagem de {image_url} com o nome {image_name}\")\n",
    "        \n",
    "        # Extrair domínio e caminho da URL\n",
    "        parsed_url = urlparse(image_url)\n",
    "        domain = parsed_url.netloc\n",
    "        path = parsed_url.path\n",
    "\n",
    "        \n",
    "        headers = {\n",
    "            'Host': domain,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br, zstd',\n",
    "            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7,ru;q=0.6,es;q=0.5',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Referer': referer,\n",
    "            'Sec-Ch-Ua': '\"Not/A)Brand\";v=\"8\", \"Chromium\";v=\"126\", \"Google Chrome\";v=\"126\"',\n",
    "            'Sec-Ch-Ua-Mobile': '?0',\n",
    "            'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'same-origin',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',\n",
    "            'Cookie': 'ajs_anonymous_id=a03ba2e0-6658-4732-bca0-5c82a729610d; blueID=158f8c36-f884-4eeb-9dd2-41ddb36954f2; _fbp=fb.2.1719565497536.945089496421286543; _tt_enable_cookie=1; _ttp=yDIqAjbcrTkStMgfYKWUtQqV7qI; _scid=fe8f87fb-9164-4128-931f-6db8349a5902; _pin_unauth=dWlkPU16QmxZV015TVRndE16QmhOUzAwWmpRekxUZzROakF0Wm1VeFlUSTROREE1WXpJeg; _hjSessionUser_257734=eyJpZCI6IjZlYmVkNTczLWRlNDItNTljNy1iYjBkLWRjMzQxYzg1ZThkZiIsImNyZWF0ZWQiOjE3MTk1NjU0OTc1ODksImV4aXN0aW5nIjp0cnVlfQ==; blueULC=blue; OptanonAlertBoxClosed=2024-06-28T10:07:39.896Z; _gac_UA-58792402-1=1.1719578340.CjwKCAjwvvmzBhA2EiwAtHVrbx5PuZ-Y6ZmDRgxSQAE3bhfdMkfQ6qO_h_InopnIh9t314DXR2UOdhoCJPMQAvD_BwE; origem=adwords; _rtbhouse_source_=AdWords; anonymousClient=true; _gcl_aw=GCL.1719608537.CjwKCAjwvvmzBhA2EiwAtHVrb7a4Gf1uUaMKzAcQ623DOKsXA0ynRueURMbfhKPxXrTvmq46s3W5ZBoCXV8QAvD_BwE; _gcl_gs=2.1.k1$i1719608535; _gid=GA1.3.1821486782.1720470235; _ScCbts=%5B%5D; petzCarrinho=true; _hjSession_257734=eyJpZCI6IjAxN2UxZjI1LWU1OTUtNDVmYS1hNmUxLWJjYjlmZjQxOWQ1NSIsImMiOjE3MjA2OTYxODM2ODUsInMiOjAsInIiOjAsInNiIjowLCJzciI6MCwic2UiOjAsImZzIjowLCJzcCI6MX0=; OptanonConsent=isGpcEnabled=0&datestamp=Thu+Jul+11+2024+08%3A13%3A33+GMT-0300+(Hor%C3%A1rio+Padr%C3%A3o+de+Bras%C3%ADlia)&version=202404.1.0&browserGpcFlag=0&isIABGlobal=false&hosts=&consentId=5d44d058-e0ca-4634-af6a-6ae970855f35&interactionCount=2&isAnonUser=1&landingPath=NotLandingPage&groups=C0001%3A1%2CC0003%3A1%2CC0002%3A1%2CC0004%3A1&AwaitingReconsent=false&intType=1&geolocation=BR%3BRJ; _gat=1; _ga_JB2JHD7FCJ=GS1.1.1720696191.1.1.1720696414.59.0.0; _ga=GA1.1.604824160.1719565115; _uetsid=006ad4a03d6811ef82e9db6f4cebf44a; _uetvid=7e097870352d11ef8996917c57f63d87; _scid_r=fe8f87fb-9164-4128-931f-6db8349a5902; _derived_epik=dj0yJnU9SHFvU3E2eWRKWGNSMTJIdUJUV2xrWUdhOUxCdGtIZGYmbj1mUTRSTnZ3d0NFWm9fQW0taEVGUGJnJm09MSZ0PUFBQUFBR2FQdm1FJnJtPTEmcnQ9QUFBQUFHYVB2bUUmc3A9Mg; cto_bundle=41bqJl9POEdtRUtaTE12dTVMYjVqT2xtMDI0bHIlMkZyRjJKcFlzc2JmN0R6R2xhMVF4ZWE1WmwlMkZwY0s5dDRMZjg2bGw3Y3EzSTlzVnc3MGo5MWN6N3Y0TnpqWHpZY1VWbXhaZGZhQ29FTDZob1RIVHFBVjhGQkNuRXZUJTJCVDBlc0MxVXdXZGo2M3Z6JTJGSUVTUjVHYU5wM09BRUVKbyUyQjU1djBQZnYwa1I3Z0pzcVAyTUQwWm8xMSUyRlBiUFBISDQ4JTJGM1c5S0xCOXZ5OTk5RCUyRnRQa2JObmtvSUZCdGNyUSUzRCUzRA; ab.storage.sessionId.8160f757-f57f-4d17-9cfb-44c82d5e8f64=%7B%22g%22%3A%22c7472c1b-0387-fd36-9939-a2bb242a337d%22%2C%22e%22%3A1720696445818%2C%22c%22%3A1720696414673%2C%22l%22%3A1720696415818%7D'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(image_url, headers=headers)\n",
    "            response.raise_for_status()  # Levanta um erro para códigos de status HTTP 4xx/5xx\n",
    "            \n",
    "            # Verificar se o conteúdo da resposta não está vazio\n",
    "            if not response.content:\n",
    "                print(f\"Falha ao baixar a imagem {image_name}. O conteúdo está vazio.\")\n",
    "                continue\n",
    "            \n",
    "            with open(image_name, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Imagem {image_name} baixada com sucesso.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Falha ao baixar a imagem {image_name}. Erro: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_images_and_update_json(json_file_path):\n",
    "    print(\"Iniciando a função save_images_and_update_json\")\n",
    "    \n",
    "    # Verificar se o arquivo JSON existe, se não, criar um novo arquivo com estrutura básica\n",
    "    if not os.path.exists(json_file_path):\n",
    "        os.makedirs(os.path.dirname(json_file_path), exist_ok=True)\n",
    "        with open(json_file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump({\"marcas\": []}, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Arquivo {json_file_path} criado.\")\n",
    "    else:\n",
    "        print(f\"Arquivo {json_file_path} encontrado.\")\n",
    "\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        print(f\"Arquivo {json_file_path} carregado com sucesso.\")\n",
    "    \n",
    "    for marca in data['marcas']:\n",
    "        print(f\"Processando marca: {marca['name']}\")\n",
    "        for produto in marca['produtos']:\n",
    "            print(f\"Processando produto: {produto['name']} (SKU: {produto['sku']})\")\n",
    "            sku = produto['sku']\n",
    "            variations = produto['variations']\n",
    "            link = produto['link']\n",
    "            domain = urlparse(link).netloc\n",
    "            \n",
    "            # Obter HTML do produto\n",
    "            html_content = get_html_with_requests(produto['link'])\n",
    "            if not html_content:\n",
    "                print(f\"Falha ao obter HTML para o produto: {produto['name']} (SKU: {sku})\")\n",
    "                continue\n",
    "            \n",
    "            # Extrair imagens\n",
    "            standard_images, variation_images = extract_images(html_content, sku, variations)\n",
    "            print(f\"Imagens extraídas para o produto: {produto['name']} (SKU: {sku})\")\n",
    "            \n",
    "            # Formatar imagens\n",
    "            formatted_images = format_images(sku, standard_images, variation_images)\n",
    "            print(f\"Imagens formatadas para o produto: {produto['name']} (SKU: {sku})\")\n",
    "            \n",
    "            # Baixar imagens padrão\n",
    "            download_images(standard_images, sku, referer=domain)\n",
    "            print(f\"Imagens padrão baixadas para o produto: {produto['name']} (SKU: {sku})\")\n",
    "            \n",
    "            # Baixar imagens das variações\n",
    "            for barcode, images in variation_images.items():\n",
    "                download_images(images, barcode)\n",
    "                print(f\"Imagens da variação {barcode} baixadas para o produto: {produto['name']} (SKU: {sku})\")\n",
    "            \n",
    "            # Atualizar informações de imagens no produto\n",
    "            produto['images'] = formatted_images\n",
    "            print(f\"Informações de imagens atualizadas para o produto: {produto['name']} (SKU: {sku})\")\n",
    "    \n",
    "    # Salvar as atualizações no arquivo JSON\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Arquivo {json_file_path} atualizado com sucesso.\")\n",
    "\n",
    "# Chamar a função para salvar imagens e atualizar o JSON\n",
    "save_images_and_update_json('produtos.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
