{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install packaging  beautifulsoup4 webdriver-manager selenium  undetected-chromedriver selenium-stealth setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re  # Importar o módulo re para expressões regulares\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium_stealth import stealth\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import setuptools\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_html_with_requests(url):\n",
    "     # Configurar os cabeçalhos\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "  \n",
    "    headers = {\n",
    "      \n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br, zstd',\n",
    "        'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7,ru;q=0.6,es;q=0.5',\n",
    "        'Cookie': 'ajs_anonymous_id=a03ba2e0-6658-4732-bca0-5c82a729610d; blueID=158f8c36-f884-4eeb-9dd2-41ddb36954f2; _fbp=fb.2.1719565497536.945089496421286543; _tt_enable_cookie=1; _ttp=yDIqAjbcrTkStMgfYKWUtQqV7qI; _scid=fe8f87fb-9164-4128-931f-6db8349a5902; _pin_unauth=dWlkPU16QmxZV015TVRndE16QmhOUzAwWmpRekxUZzROakF0Wm1VeFlUSTROREE1WXpJeg; _hjSessionUser_257734=eyJpZCI6IjZlYmVkNTczLWRlNDItNTljNy1iYjBkLWRjMzQxYzg1ZThkZiIsImNyZWF0ZWQiOjE3MTk1NjU0OTc1ODksImV4aXN0aW5nIjp0cnVlfQ==; __utmz=other; blueULC=blue; OptanonAlertBoxClosed=2024-06-28T10:07:39.896Z; _gac_UA-58792402-1=1.1719578340.CjwKCAjwvvmzBhA2EiwAtHVrbx5PuZ-Y6ZmDRgxSQAE3bhfdMkfQ6qO_h_InopnIh9t314DXR2UOdhoCJPMQAvD_BwE; origem=adwords; _rtbhouse_source_=AdWords; anonymousClient=true; _gcl_aw=GCL.1719608537.CjwKCAjwvvmzBhA2EiwAtHVrb7a4Gf1uUaMKzAcQ623DOKsXA0ynRueURMbfhKPxXrTvmq46s3W5ZBoCXV8QAvD_BwE; _gcl_gs=2.1.k1$i1719608535; _gid=GA1.3.1821486782.1720470235; _ScCbts=%5B%5D; petzCarrinho=true; JSESSIONID=9E56A4F6251336BC2EEC9640328DF1CE; _hjSession_257734=eyJpZCI6ImRmYjFhNzkxLTcwMDAtNDc5ZC1hZDA2LTE4NGYwZGMwOTQ3MSIsImMiOjE3MjA2MzEyODk1MzEsInMiOjAsInIiOjAsInNiIjowLCJzciI6MCwic2UiOjAsImZzIjowLCJzcCI6MH0=; OptanonConsent=isGpcEnabled=0&datestamp=Wed+Jul+10+2024+14%3A33%3A28+GMT-0300+(Hor%C3%A1rio+Padr%C3%A3o+de+Bras%C3%ADlia)&version=202404.1.0&browserGpcFlag=0&isIABGlobal=false&hosts=&consentId=5d44d058-e0ca-4634-af6a-6ae970855f35&interactionCount=2&isAnonUser=1&landingPath=NotLandingPage&groups=C0001%3A1%2CC0003%3A1%2CC0002%3A1%2CC0004%3A1&AwaitingReconsent=false&intType=1&geolocation=BR%3BRJ; AWSALB=hZab3iIPYsD0MIUknEJjbESldllV2XbXHeW6ABubXDqQC1w+fB9ShoCYGJIR2z0ndrcA5n/bdu5qP2SU+sm1uOdvT/LotUC6UCm2meSI5Mm4DIVUR9N7D5oqkEID; AWSALBCORS=hZab3iIPYsD0MIUknEJjbESldllV2XbXHeW6ABubXDqQC1w+fB9ShoCYGJIR2z0ndrcA5n/bdu5qP2SU+sm1uOdvT/LotUC6UCm2meSI5Mm4DIVUR9N7D5oqkEID; _ga_JB2JHD7FCJ=GS1.1.1720631306.23.1.1720632809.57.0.0; _ga=GA1.1.604824160.1719565115; _uetsid=006ad4a03d6811ef82e9db6f4cebf44a; _uetvid=7e097870352d11ef8996917c57f63d87; _scid_r=fe8f87fb-9164-4128-931f-6db8349a5902; _derived_epik=dj0yJnU9OVpXTXl0T0xlTmRISjQ3aGwtVFQ1SWZjVmJuT0JiaFkmbj1zc3Z6NkNFQXNmdnZHWmFDdEpIX093Jm09MSZ0PUFBQUFBR2FPeGVzJnJtPTEmcnQ9QUFBQUFHYU94ZXMmc3A9Mg; cto_bundle=Eb4Xml9POEdtRUtaTE12dTVMYjVqT2xtMDIxaDE5blJXQWRHRkZXVEJlcUdNOTdFRGNwdGtMeWslMkZGVnBENzMwM0JYMDVmQkdIeVQ4NW1qMW9laEslMkZRQnl3dzElMkZPWFUlMkZmTFJxNGdLeXUlMkZTMTRqZyUyRndXSFZ4dEsxS1hyMk1ZeGpOQVU4bmU1bkFZY1lwcHp2M3NVZ1FWQWNmYmRYS0RIc1N1ZzFBcWRZcWNlOVcxRkVaeVZGd0YzQUxFMWglMkZheXhJa0cyJTJGdUg0NiUyQk92VCUyRnVZZmtxQWN6QVAlMkZzdyUzRCUzRA; ab.storage.sessionId.8160f757-f57f-4d17-9cfb-44c82d5e8f64=%7B%22g%22%3A%221c387921-d350-d759-9bb8-04a9e46b2359%22%2C%22e%22%3A1720632840493%2C%22c%22%3A1720632809379%2C%22l%22%3A1720632810493%7D; RT=\"z=1&dm=www.petz.com.br&si=92ca62e5-ee3d-405f-ab43-07f1cffe3709&ss=lyg3fxm8&sl=6&tt=cia&obo=2&rl=1&ld=zeiw&r=ke1m2ztx&ul=zeix\"',\n",
    "        'Priority': 'u=0, i',\n",
    "        'Referer': f\"{domain}/marcas\",\n",
    "        'Sec-Ch-Ua': '\"Not/A)Brand\";v=\"8\", \"Chromium\";v=\"126\", \"Google Chrome\";v=\"126\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Configurar o driver do Selenium\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Executar o navegador em modo headless\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--disable-infobars')\n",
    "    options.add_argument('--disable-extensions')\n",
    "    options.add_argument('--profile-directory=Default')\n",
    "    options.add_argument('--user-data-dir=~/.config/google-chrome')\n",
    "\n",
    "    driver = uc.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    # Aplicar técnicas de camuflagem\n",
    "    stealth(driver,\n",
    "            languages=[\"en-US\", \"en\"],\n",
    "            vendor=\"Google Inc.\",\n",
    "            platform=\"Win32\",\n",
    "            webgl_vendor=\"Intel Inc.\",\n",
    "            renderer=\"Intel Iris OpenGL Engine\",\n",
    "            fix_hairline=True,\n",
    "            )\n",
    "\n",
    "    # Adicionar os cabeçalhos à solicitação\n",
    "    driver.request_interceptor = lambda request: [request.headers.update(headers)]\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            # Rolar até o final da página\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(5)  # Esperar o carregamento da página\n",
    "            \n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        return html_content\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter HTML com Selenium: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_product_details(product_url, last_sku):\n",
    "    parsed_url = urlparse(product_url)\n",
    "    domain = parsed_url.netloc\n",
    "\n",
    "    print(f\"Extraindo detalhes do produto da URL: {product_url}\")\n",
    "    html_content = get_html_with_requests(product_url)\n",
    "    if not html_content:\n",
    "        print(f\"Falha ao obter HTML para o produto: {product_url}\")\n",
    "        return {}, last_sku\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Extrair categoria principal e subcategorias\n",
    "    breadcrumb = soup.find('nav', {'aria-label': 'breadcrumb'})\n",
    "    categories = []\n",
    "    if breadcrumb:\n",
    "        breadcrumb_items = breadcrumb.find_all('span', class_='breadcrumb-item')\n",
    "        if breadcrumb_items:\n",
    "            main_category = breadcrumb_items[1].find('meta', itemprop='name')['content']\n",
    "            subcategories = [item.find('meta', itemprop='name')['content'] for item in breadcrumb_items[2:]]\n",
    "            categories = {\n",
    "                \"name\": main_category,\n",
    "                \"link\": \"\",  # Link não disponível\n",
    "                \"subcategories\": [{\"name\": sub, \"link\": \"\"} for sub in subcategories]\n",
    "            }\n",
    "            print(f\"Categorias encontradas: {categories}\")\n",
    "        else:\n",
    "            print(\"Nenhuma categoria encontrada no breadcrumb.\")\n",
    "    else:\n",
    "        print(\"Breadcrumb não encontrado.\")\n",
    "    \n",
    "    # Extrair código de barras\n",
    "    barcode_element = soup.find('span', {'id': 'product-code'})\n",
    "    if barcode_element:\n",
    "        barcode = barcode_element.text.replace('Código: ', '').strip()\n",
    "        print(f\"Código de barras encontrado: {barcode}\")\n",
    "    else:\n",
    "        barcode = None\n",
    "        print(\"Código de barras não encontrado.\")\n",
    "    \n",
    "       # Extrair marca principal\n",
    "    brand_element = soup.find('a', {'itemprop': 'brand'})\n",
    "    if brand_element:\n",
    "        brand = brand_element.find('span', {'itemprop': 'name'}).text.strip()\n",
    "        print(f\"Marca encontrada: {brand}\")\n",
    "    else:\n",
    "        brand = None\n",
    "        print(\"Marca não encontrada.\")\n",
    "    \n",
    "    # Extrair submarca\n",
    "    subbrand_element = None\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        if a_tag.get('itemprop') is None and a_tag.find('span', class_='blue'):\n",
    "            subbrand_element = a_tag\n",
    "            break\n",
    "    \n",
    "    if subbrand_element:\n",
    "        subbrand = subbrand_element.find('span', class_='blue').text.strip()\n",
    "        print(f\"Submarca encontrada: {subbrand}\")\n",
    "    else:\n",
    "        subbrand = None\n",
    "        print(\"Submarca não encontrada.\")\n",
    "    \n",
    "    # Extrair descrição do produto\n",
    "    description_element = soup.find('section', {'class': 'description', 'id': 'description'})\n",
    "    if description_element:\n",
    "        description = description_element.find('div', class_='spec-content').text.strip().split(\"-\")\n",
    "        print(f\"Descrição encontrada: {description}\")\n",
    "    else:\n",
    "        description = None\n",
    "        print(\"Descrição não encontrada.\")\n",
    "\n",
    "\n",
    "        description= description.texto.split(\"-\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Extrair especificações do produto\n",
    "    specifications = []\n",
    "    specifications_container = soup.find('section', id='specifications')\n",
    "    if specifications_container:\n",
    "        spec_items = specifications_container.find_all('li')\n",
    "      \n",
    "\n",
    "        for item in spec_items:\n",
    "            spec_key = item.find('span', class_='spec-key').text.strip()\n",
    "            spec_value = item.find('span', class_='spec-value').text.strip()\n",
    "            specifications.append({\n",
    "                'key': spec_key,\n",
    "                'value': spec_value\n",
    "            })\n",
    "        print(f\"Especificações encontradas: {specifications}\")\n",
    "    else:\n",
    "        print(\"Especificações não encontradas.\")\n",
    "    \n",
    "    # Extrair variações do produto\n",
    "    variations = []\n",
    "    variations_container = soup.find('div', {'id': 'popupVariacoes'})\n",
    "    if variations_container:\n",
    "        print(\"Container de variações encontrado.\")\n",
    "        print(f\"HTML das variações:\\n{variations_container.prettify()}\\n\")\n",
    "        for variation in variations_container.find_all('div', class_='variacao-item'):\n",
    "            variation_name = variation.find('div', class_='item-name').text.strip()\n",
    "            price = variation.find('div', class_='modal-item-price').find('b').text.strip()\n",
    "            price = re.sub(r'[^\\d]', '', price)  # Remover caracteres não numéricos\n",
    "            code = variation['data-code']\n",
    "            link_variation = domain + variation['data-urlvariacao']\n",
    "            last_sku += 1\n",
    "            print(f\"Extraindo detalhes da variação: {variation_name}, Preço: {price},   code: {  code}, Link: {link_variation}, SKU: {last_sku}\")\n",
    "            \n",
    "            variations.append({\n",
    "                'name': variation_name,\n",
    "                'price': price,\n",
    "                'link': link_variation,\n",
    "                'code': code,\n",
    "                'sku': last_sku\n",
    "            })\n",
    "        print(f\"Variações encontradas: {variations}\")\n",
    "    else:\n",
    "        print(\"Nenhuma variação encontrada.\")\n",
    "        # Verificar se o preço está diretamente no HTML\n",
    "        price_element = soup.find('div', class_='current-price-left')\n",
    "        if price_element:\n",
    "            price = price_element.find('strong').text.strip()\n",
    "            price = re.sub(r'[^\\d]', '', price)  # Remover caracteres não numéricos\n",
    "            last_sku += 1\n",
    "            print(f\"Preço encontrado: {price}, SKU: {last_sku}\")\n",
    "            \n",
    "            variations.append({\n",
    "                'name': 'default',\n",
    "                'price': price,\n",
    "                'code': barcode,\n",
    "                'link': product_url,\n",
    "                'sku': last_sku\n",
    "            })\n",
    "        else:\n",
    "            print(\"Preço não encontrado.\")\n",
    "    \n",
    "    product_details = {\n",
    "        'categories': categories,\n",
    "        'code': barcode,\n",
    "        'brand': {\n",
    "            'name': brand,\n",
    "            'subbrand': {\n",
    "                'name': subbrand\n",
    "            } if subbrand else None\n",
    "        },\n",
    "        'description': description,\n",
    "        'specifications': specifications,\n",
    "        'variations': variations\n",
    "    }\n",
    "    \n",
    "    print(f\"Detalhes do produto extraídos: {product_details}\")\n",
    "    return product_details, last_sku\n",
    "\n",
    "def extract_products(html_content, marca_name, marca_link, last_sku):\n",
    "      # Carregar produtos_data antes de usá-la\n",
    "    with open('marcas_selecionadas.json', 'r', encoding='utf-8') as f:\n",
    "        produtos_data = json.load(f)\n",
    "\n",
    "\n",
    "        \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    product_elements = soup.select('li.card-product.card-product-showcase[itemtype=\"http://schema.org/Product\"]')\n",
    "    \n",
    "    print(f\"Encontrados {len(product_elements)} elementos de produto para a marca '{marca_name}'\")\n",
    "    \n",
    "    products = []\n",
    "    seen_product_names = set() # Conjunto para rastrear nomes de produtos já processados\n",
    "    for product in product_elements:\n",
    "        json_data = product.find('textarea', class_='jsonGa')\n",
    "        if json_data:\n",
    "            json_data = json_data.text.strip()\n",
    "            product_info = json.loads(json_data)\n",
    "        else:\n",
    "            print(\"Elemento jsonGa não encontrado.\")\n",
    "            continue\n",
    "        \n",
    "        name = product_info.get('name')\n",
    "\n",
    "\n",
    "\n",
    "  # Verificar se o produto já foi processado\n",
    "        if name in seen_product_names:\n",
    "            print(f\"Produto duplicado encontrado e ignorado: {name}\")\n",
    "            continue\n",
    "        \n",
    "        seen_product_names.add(name)  # Adicionar o nome do produto ao conjunto\n",
    "        \n",
    "        # Verificar se o produto já existe no arquivo\n",
    "        existing_product = next((p for m in produtos_data[\"marcas\"] if m[\"name\"] == marca_name for p in m[\"produtos\"] if p[\"name\"] == name), None)\n",
    "        if existing_product:\n",
    "         print(f\"Produto '{name}' já existe no arquivo e será ignorado.\")\n",
    "         continue\n",
    "\n",
    "\n",
    "        link_element = product.find('meta', itemprop='url')\n",
    "        link = link_element['content'] if link_element else None\n",
    "        \n",
    "        if not link:\n",
    "            print(f\"Link não encontrado para o produto: {name}\")\n",
    "            continue\n",
    "        \n",
    "        link = 'https://' + link\n",
    "        print(f\"Processando produto: {name}, URL: {link}\")\n",
    "        price = product_info.get('price')\n",
    "        product_id = product_info.get('id')\n",
    "        category = product_info.get('category')\n",
    "        brand = product_info.get('brand')\n",
    "        price_for_subs = product_info.get('priceForSubs')\n",
    "        hide_subscriber_discount_price = product_info.get('hideSubscriberDiscountPrice')\n",
    "        \n",
    "        product_data = {\n",
    "            \"name\": name,\n",
    "            \"link\": link,\n",
    "            \"price\": price,\n",
    "            \"id\": product_id,\n",
    "            \"category\": category,\n",
    "            \"brand\": brand,\n",
    "            \"priceForSubs\": price_for_subs,\n",
    "            \"hideSubscriberDiscountPrice\": hide_subscriber_discount_price\n",
    "        }\n",
    "        \n",
    "        # Obter detalhes adicionais do produto\n",
    "        product_details, last_sku = extract_product_details(link, last_sku)\n",
    "        if product_details:\n",
    "            product_data.update(product_details)\n",
    "        \n",
    "        products.append(product_data)\n",
    "        \n",
    "        # Inserir o produto no arquivo marcas_selecionadas.json\n",
    "        with open('marcas_selecionadas.json', 'r', encoding='utf-8') as f:\n",
    "            produtos_data = json.load(f)\n",
    "        \n",
    "        # Verifica se a marca já existe no produtos_data\n",
    "        existing_brand = next((m for m in produtos_data[\"marcas\"] if m[\"name\"] == marca_name), None)\n",
    "        \n",
    "        if existing_brand:\n",
    "            existing_brand[\"produtos\"].append(product_data)\n",
    "        else:\n",
    "            marca_data = {\n",
    "                \"name\": marca_name,\n",
    "                \"link\": marca_link,\n",
    "                \"produtos\": [product_data]\n",
    "            }\n",
    "            produtos_data[\"marcas\"].append(marca_data)\n",
    "        \n",
    "        # Salvar o arquivo marcas_selecionadas.json\n",
    "        with open('marcas_selecionadas.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(produtos_data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Produto '{name}' adicionado à marca '{marca_name}'\")\n",
    "    \n",
    "    return products, last_sku\n",
    "\n",
    "def process_brand(marca, last_sku):\n",
    "    url = marca['link']\n",
    "    print(f\"Processando marca: {marca['name']} - URL: {url}\")\n",
    "    try:\n",
    "        html_content = get_html_with_requests(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter HTML para a marca {marca['name']} - URL: {url} - Erro: {e}\")\n",
    "        return False, last_sku\n",
    "    \n",
    "    if html_content:\n",
    "        products, last_sku = extract_products(html_content, marca['name'], url, last_sku)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if products is None:\n",
    "            return False, last_sku\n",
    "        print(f\"{len(products)} produtos processados para a marca '{marca['name']}'\")\n",
    "    else:\n",
    "        print(f\"Falha ao obter HTML para a marca {marca['name']} - URL: {url}\")\n",
    "        return False, last_sku\n",
    "    \n",
    "    return True, last_sku\n",
    "\n",
    "def main():\n",
    "    print(\"Iniciando o processo principal\")\n",
    "\n",
    "\n",
    " # Verificar se o arquivo marcas_selecionadas.json existe\n",
    "    if os.path.exists('marcas_selecionadas.json'):\n",
    "        try:\n",
    "            with open('marcas_selecionadas.json', 'r', encoding='utf-8') as f:\n",
    "                produtos_data = json.load(f)\n",
    "            print(\"Arquivo marcas_selecionadas.json carregado\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Erro ao carregar marcas_selecionadas.json: {e}\")\n",
    "            return\n",
    "    else:\n",
    "\n",
    "       with open('marcas.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        print(\"Arquivo marcas.json carregado\")\n",
    "\n",
    "        # Criar o arquivo marcas_selecionadas.json se não existir\n",
    "        produtos_data = {\"marcas\":  data['subcategories']}\n",
    "        with open('marcas_selecionadas.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(produtos_data, f, ensure_ascii=False, indent=4)\n",
    "        print(\"Arquivo marcas_selecionadas.json criado\")\n",
    "\n",
    "\n",
    "    # Carregar o último SKU processado\n",
    "    last_sku = 0\n",
    "    try:\n",
    "        with open('marcas_selecionadas.json', 'r', encoding='utf-8') as f:\n",
    "            produtos_data = json.load(f)\n",
    "        for marca in produtos_data[\"marcas\"]:\n",
    "            for produto in marca[\"produtos\"]:\n",
    "                for variacao in produto.get(\"variations\", []):\n",
    "                    if variacao[\"sku\"] > last_sku:\n",
    "                        last_sku = variacao[\"sku\"]\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Erro ao carregar produtos.json: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Processar todas as marcas\n",
    "    for marca in  produtos_data['marcas']:\n",
    "      \n",
    "        _, last_sku = process_brand(marca, last_sku)\n",
    "\n",
    "\n",
    "    print(\"Processo concluído.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
